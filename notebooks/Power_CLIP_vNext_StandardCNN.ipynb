{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ Power-CLIP vNext: Standard CNN Encoder\n",
                "\n",
                "**Â∞çÁÖßÂØ¶È©ó**: Áî®Ê®ôÊ∫ñ CNN (CLIP-style) Âèñ‰ª£ HTF-CNN\n",
                "\n",
                "| Â∞çÊØî | HTF-CNN | Standard CNN |\n",
                "|------|---------|-------------|\n",
                "| Branches | 3 (time+freq+stats) | 1 (signal only) |\n",
                "| FFT | ‚úÖ | ‚ùå |\n",
                "| Stats | ‚úÖ | ‚ùå |\n",
                "\n",
                "**ÁâàÊú¨**: 2025-12-31 vNext-StandardCNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìÇ Setup\n",
                "from google.colab import drive, userdata\n",
                "import os, json, torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "import random\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "BASE_PATH = '/content/drive/MyDrive/Reserach/REDD_Dataset'\n",
                "os.chdir(BASE_PATH)\n",
                "\n",
                "SEED = 42\n",
                "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'üî• Device: {DEVICE}')\n",
                "\n",
                "EPOCHS = 20\n",
                "LR = 1e-3\n",
                "BATCH_SIZE = 32\n",
                "TEMPERATURE = 0.07"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìã Attribute Ontology (20 prototypes)\n",
                "\n",
                "ATTRIBUTES = {\n",
                "    'PS1': {'tier': 'power_shape', 'name': 'constant_high', 'text': 'sustained high power consumption above 500W'},\n",
                "    'PS2': {'tier': 'power_shape', 'name': 'constant_low', 'text': 'sustained low power consumption under 200W'},\n",
                "    'PS3': {'tier': 'power_shape', 'name': 'cyclic', 'text': 'power cycles on and off periodically'},\n",
                "    'PS4': {'tier': 'power_shape', 'name': 'burst', 'text': 'short bursts of high power'},\n",
                "    'PS5': {'tier': 'power_shape', 'name': 'fluctuating', 'text': 'continuously fluctuating power levels'},\n",
                "    'T1': {'tier': 'temporal', 'name': 'long_operation', 'text': 'operates continuously for 30+ minutes'},\n",
                "    'T2': {'tier': 'temporal', 'name': 'short_usage', 'text': 'used for less than 10 minutes at a time'},\n",
                "    'T3': {'tier': 'temporal', 'name': 'periodic_cycle', 'text': 'regular on-off cycles every 15-30 minutes'},\n",
                "    'T4': {'tier': 'temporal', 'name': 'instantaneous', 'text': 'very brief operation under 5 minutes'},\n",
                "    'L1': {'tier': 'load_type', 'name': 'resistive', 'text': 'purely resistive heating load with stable power'},\n",
                "    'L2': {'tier': 'load_type', 'name': 'inductive_motor', 'text': 'inductive motor load with startup surge'},\n",
                "    'L3': {'tier': 'load_type', 'name': 'switching', 'text': 'switching power supply with high frequency noise'},\n",
                "    'L4': {'tier': 'load_type', 'name': 'heating_element', 'text': 'resistive heating element with thermal cycling'},\n",
                "    'L5': {'tier': 'load_type', 'name': 'compressor', 'text': 'compressor motor with periodic start-stop'},\n",
                "    'P1': {'tier': 'power_level', 'name': 'very_high', 'text': 'power consumption above 1500W'},\n",
                "    'P2': {'tier': 'power_level', 'name': 'high', 'text': 'power consumption between 500W and 1500W'},\n",
                "    'P3': {'tier': 'power_level', 'name': 'medium', 'text': 'power consumption between 100W and 500W'},\n",
                "    'P4': {'tier': 'power_level', 'name': 'low', 'text': 'power consumption below 100W'},\n",
                "    'S1': {'tier': 'stability', 'name': 'stable', 'text': 'very stable and consistent power draw'},\n",
                "    'S2': {'tier': 'stability', 'name': 'variable', 'text': 'highly variable and unpredictable power'},\n",
                "}\n",
                "\n",
                "ATTR_IDS = list(ATTRIBUTES.keys())\n",
                "ATTR_TEXTS = [ATTRIBUTES[k]['text'] for k in ATTR_IDS]\n",
                "\n",
                "DEVICE_ATTRS = {\n",
                "    'fridge': ['L5', 'T3', 'P4', 'PS3', 'S1'],\n",
                "    'microwave': ['L1', 'T4', 'P1', 'PS4', 'S1'],\n",
                "    'dish washer': ['L2', 'L4', 'T1', 'P2', 'PS5'],\n",
                "    'washer dryer': ['L2', 'L4', 'T1', 'P1', 'PS5', 'S2'],\n",
                "    'electric stove': ['L1', 'T2', 'P1', 'PS1', 'S1'],\n",
                "    'electric space heater': ['L4', 'T1', 'P2', 'PS1', 'S1'],\n",
                "}\n",
                "\n",
                "ALL_DEVICES = list(DEVICE_ATTRS.keys())\n",
                "SHORT = {'fridge': 'F', 'microwave': 'M', 'dish washer': 'D', \n",
                "         'washer dryer': 'W', 'electric stove': 'S', 'electric space heater': 'H'}\n",
                "\n",
                "print(f'üìã Attributes: {len(ATTR_IDS)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title ‚ö° Standard CNN Encoder (CLIP-style)\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "class StandardCNNEncoder(nn.Module):\n",
                "    \"\"\"\n",
                "    Standard 1D CNN - CLIP style (single branch, no fancy fusion)\n",
                "    Pure signal-based encoding, no FFT or stats\n",
                "    \"\"\"\n",
                "    def __init__(self, embed_dim=384):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            # Block 1: [B, 1, 60] -> [B, 64, 15]\n",
                "            nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3),\n",
                "            nn.BatchNorm1d(64),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool1d(2),\n",
                "            \n",
                "            # Block 2: [B, 64, 15] -> [B, 128, 7]\n",
                "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm1d(128),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool1d(2),\n",
                "            \n",
                "            # Block 3: [B, 128, 7] -> [B, 256, 1]\n",
                "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.ReLU(),\n",
                "            nn.AdaptiveAvgPool1d(1)\n",
                "        )\n",
                "        \n",
                "        self.projection = nn.Sequential(\n",
                "            nn.Linear(256, embed_dim),\n",
                "            nn.LayerNorm(embed_dim)\n",
                "        )\n",
                "    \n",
                "    def forward(self, signal):\n",
                "        # Normalize signal\n",
                "        mean = signal.mean(1, keepdim=True)\n",
                "        std = signal.std(1, keepdim=True) + 1e-6\n",
                "        x = (signal - mean) / std\n",
                "        \n",
                "        x = x.unsqueeze(1)      # [B, 1, 60]\n",
                "        x = self.conv(x)         # [B, 256, 1]\n",
                "        x = x.squeeze(-1)        # [B, 256]\n",
                "        x = self.projection(x)   # [B, 384]\n",
                "        return x\n",
                "\n",
                "model = StandardCNNEncoder()\n",
                "print(f'‚úÖ Standard CNN: {sum(p.numel() for p in model.parameters()):,} params')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìä Load All Data\n",
                "\n",
                "all_samples = []\n",
                "ws = 60\n",
                "with open('combination_labels.jsonl') as f:\n",
                "    lines = f.readlines()\n",
                "\n",
                "for i in tqdm(range(0, len(lines)-ws, ws), desc='Loading'):\n",
                "    chunk = [json.loads(lines[j]) for j in range(i, i+ws)]\n",
                "    act = set(chunk[ws//2]['active_appliances']) & set(ALL_DEVICES)\n",
                "    if not act: continue\n",
                "    \n",
                "    attr_set = set()\n",
                "    for d in act:\n",
                "        attr_set.update(DEVICE_ATTRS.get(d, []))\n",
                "    if not attr_set: continue\n",
                "    \n",
                "    signal = np.array([c['aggregate_power'] for c in chunk], dtype=np.float32)\n",
                "    attr_mask = np.zeros(len(ATTR_IDS), dtype=np.float32)\n",
                "    for attr in attr_set:\n",
                "        attr_mask[ATTR_IDS.index(attr)] = 1.0\n",
                "    \n",
                "    all_samples.append({\n",
                "        'signal': signal,\n",
                "        'attr_mask': attr_mask,\n",
                "        'devices': list(act),\n",
                "        'attrs': list(attr_set)\n",
                "    })\n",
                "\n",
                "print(f'üìä Total samples: {len(all_samples)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üéØ LOO Attribute Dataset\n",
                "\n",
                "class LOOAttrDataset(Dataset):\n",
                "    def __init__(self, samples, left_out_device, is_train=True):\n",
                "        if is_train:\n",
                "            self.samples = [s for s in samples if left_out_device not in s['devices']]\n",
                "        else:\n",
                "            self.samples = [s for s in samples \n",
                "                           if left_out_device in s['devices'] and len(s['devices']) == 1]\n",
                "        self.left_out = left_out_device\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    def __getitem__(self, i):\n",
                "        s = self.samples[i]\n",
                "        return (torch.tensor(s['signal']), torch.tensor(s['attr_mask']), \n",
                "                s['devices'], s['attrs'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üî• Train & Evaluate One LOO Fold\n",
                "\n",
                "def run_loo_fold(left_out, all_samples, epochs=EPOCHS):\n",
                "    print(f'\\n{\"=\"*60}')\n",
                "    print(f'üéØ LOO StandardCNN: Leave out [{SHORT[left_out]}] = {left_out}')\n",
                "    print(f'{\"=\"*60}')\n",
                "    \n",
                "    train_ds = LOOAttrDataset(all_samples, left_out, is_train=True)\n",
                "    test_ds = LOOAttrDataset(all_samples, left_out, is_train=False)\n",
                "    \n",
                "    print(f'Train: {len(train_ds)} | Test: {len(test_ds)}')\n",
                "    \n",
                "    if len(test_ds) == 0:\n",
                "        return None\n",
                "    \n",
                "    model = StandardCNNEncoder().to(DEVICE)\n",
                "    text_model = SentenceTransformer('all-MiniLM-L6-v2').to(DEVICE)\n",
                "    text_model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        attr_embs = text_model.encode(ATTR_TEXTS, convert_to_tensor=True, device=DEVICE)\n",
                "        attr_embs = F.normalize(attr_embs, p=2, dim=1)\n",
                "    \n",
                "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
                "    \n",
                "    def collate(batch):\n",
                "        return (torch.stack([b[0] for b in batch]), \n",
                "                torch.stack([b[1] for b in batch]),\n",
                "                [b[2] for b in batch], [b[3] for b in batch])\n",
                "    \n",
                "    loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
                "                        drop_last=True, collate_fn=collate)\n",
                "    \n",
                "    # Training\n",
                "    for ep in range(epochs):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        for signals, masks, _, _ in loader:\n",
                "            signals, masks = signals.to(DEVICE), masks.to(DEVICE)\n",
                "            pe = F.normalize(model(signals), p=2, dim=1)\n",
                "            sim = (pe @ attr_embs.T) / TEMPERATURE\n",
                "            exp_sim = torch.exp(sim)\n",
                "            pos_sum = (exp_sim * masks).sum(dim=1)\n",
                "            all_sum = exp_sim.sum(dim=1)\n",
                "            loss = -torch.log(pos_sum / (all_sum + 1e-8)).mean()\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            optimizer.step()\n",
                "            total_loss += loss.item()\n",
                "        if (ep+1) % 10 == 0:\n",
                "            print(f'  Ep {ep+1}/{epochs} Loss: {total_loss/len(loader):.4f}')\n",
                "    \n",
                "    # Evaluation\n",
                "    model.eval()\n",
                "    device_ranks, attr_hits = [], []\n",
                "    target_attrs = set(DEVICE_ATTRS[left_out])\n",
                "    \n",
                "    for i in range(len(test_ds)):\n",
                "        signal, gt_mask, _, gt_attrs = test_ds[i]\n",
                "        signal = signal.unsqueeze(0).to(DEVICE)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            pe = F.normalize(model(signal), p=2, dim=1)\n",
                "            sim = (pe @ attr_embs.T).squeeze().cpu().numpy()\n",
                "        \n",
                "        activations = 1 / (1 + np.exp(-sim * 5))\n",
                "        \n",
                "        device_scores = {}\n",
                "        for dev, expected in DEVICE_ATTRS.items():\n",
                "            indices = [ATTR_IDS.index(a) for a in expected]\n",
                "            device_scores[dev] = np.mean([activations[j] for j in indices])\n",
                "        \n",
                "        sorted_devs = sorted(device_scores.items(), key=lambda x: -x[1])\n",
                "        rank = [d[0] for d in sorted_devs].index(left_out) + 1\n",
                "        device_ranks.append(rank)\n",
                "        \n",
                "        top_k_attrs = set([ATTR_IDS[j] for j in np.argsort(-activations)[:len(target_attrs)]])\n",
                "        hit = len(top_k_attrs & target_attrs) / len(target_attrs)\n",
                "        attr_hits.append(hit)\n",
                "    \n",
                "    avg_rank = np.mean(device_ranks)\n",
                "    top1 = np.mean([r == 1 for r in device_ranks]) * 100\n",
                "    top3 = np.mean([r <= 3 for r in device_ranks]) * 100\n",
                "    attr_hit_rate = np.mean(attr_hits) * 100\n",
                "    \n",
                "    print(f'üìä {SHORT[left_out]}: Rank={avg_rank:.2f}, Top-1={top1:.1f}%, Top-3={top3:.1f}%, AttrHit={attr_hit_rate:.1f}%')\n",
                "    \n",
                "    return {\n",
                "        'device': left_out,\n",
                "        'short': SHORT[left_out],\n",
                "        'n_train': len(train_ds),\n",
                "        'n_test': len(test_ds),\n",
                "        'avg_rank': float(avg_rank),\n",
                "        'top1': float(top1),\n",
                "        'top3': float(top3),\n",
                "        'attr_hit_rate': float(attr_hit_rate)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üöÄ Run All 6 LOO Folds\n",
                "\n",
                "results = []\n",
                "for device in ALL_DEVICES:\n",
                "    result = run_loo_fold(device, all_samples, epochs=EPOCHS)\n",
                "    if result:\n",
                "        results.append(result)\n",
                "\n",
                "print('\\n' + '=' * 60)\n",
                "print('üìä ALL LOO FOLDS COMPLETE (Standard CNN)')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìä Summary & Comparison\n",
                "\n",
                "print('\\n### Standard CNN vs HTF-CNN Comparison\\n')\n",
                "print('| Device | Avg Rank | Top-1 | Top-3 | Attr Hit |')\n",
                "print('|--------|----------|-------|-------|----------|')\n",
                "\n",
                "for r in results:\n",
                "    better = '‚úÖ' if r['avg_rank'] < 3.5 else '‚ùå'\n",
                "    print(f\"| {r['short']} | {r['avg_rank']:.2f} {better} | {r['top1']:.1f}% | {r['top3']:.1f}% | {r['attr_hit_rate']:.1f}% |\")\n",
                "\n",
                "avg_rank = np.mean([r['avg_rank'] for r in results])\n",
                "avg_top1 = np.mean([r['top1'] for r in results])\n",
                "avg_top3 = np.mean([r['top3'] for r in results])\n",
                "avg_attr = np.mean([r['attr_hit_rate'] for r in results])\n",
                "\n",
                "print(f'| **Avg** | **{avg_rank:.2f}** | **{avg_top1:.1f}%** | **{avg_top3:.1f}%** | **{avg_attr:.1f}%** |')\n",
                "print(f'| Random | 3.5 | 16.7% | 50% | - |')\n",
                "\n",
                "print(f'\\nüìã HTF-CNN Results (for comparison):')\n",
                "print(f'   Avg Rank: 4.28, Top-1: 7.7%, Top-3: 17.2%, Attr Hit: 27.8%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìà Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "devices = [r['short'] for r in results]\n",
                "ranks = [r['avg_rank'] for r in results]\n",
                "top3s = [r['top3'] for r in results]\n",
                "hits = [r['attr_hit_rate'] for r in results]\n",
                "\n",
                "colors = ['green' if r < 3.5 else 'red' for r in ranks]\n",
                "axes[0].bar(devices, ranks, color=colors, alpha=0.7)\n",
                "axes[0].axhline(y=3.5, color='black', linestyle='--', label='Random')\n",
                "axes[0].set_ylabel('Avg Rank (‚Üì)')\n",
                "axes[0].set_title('Standard CNN: Device Rank')\n",
                "axes[0].legend()\n",
                "\n",
                "colors = ['green' if t > 50 else 'red' for t in top3s]\n",
                "axes[1].bar(devices, top3s, color=colors, alpha=0.7)\n",
                "axes[1].axhline(y=50, color='black', linestyle='--', label='Random')\n",
                "axes[1].set_ylabel('Top-3 (%)')\n",
                "axes[1].set_title('Standard CNN: Top-3 Recall')\n",
                "axes[1].legend()\n",
                "\n",
                "axes[2].bar(devices, hits, color='blue', alpha=0.7)\n",
                "axes[2].set_ylabel('Attr Hit Rate (%)')\n",
                "axes[2].set_title('Standard CNN: Attribute Retrieval')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('stdcnn_loo_results.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üíæ Save Results\n",
                "\n",
                "final_results = {\n",
                "    'experiment': 'Standard CNN Attribute-Level LOO Zero-Shot',\n",
                "    'encoder': 'StandardCNN',\n",
                "    'epochs': EPOCHS,\n",
                "    'n_attributes': len(ATTR_IDS),\n",
                "    'folds': results,\n",
                "    'average': {\n",
                "        'rank': float(avg_rank),\n",
                "        'top1': float(avg_top1),\n",
                "        'top3': float(avg_top3),\n",
                "        'attr_hit_rate': float(avg_attr)\n",
                "    },\n",
                "    'vs_random': {\n",
                "        'rank_better': bool(avg_rank < 3.5),\n",
                "        'top3_better': bool(avg_top3 > 50)\n",
                "    },\n",
                "    'comparison': {\n",
                "        'htf_cnn_rank': 4.28,\n",
                "        'htf_cnn_top1': 7.7,\n",
                "        'htf_cnn_top3': 17.2\n",
                "    }\n",
                "}\n",
                "\n",
                "with open('stdcnn_loo_zeroshot_results.json', 'w') as f:\n",
                "    json.dump(final_results, f, indent=2)\n",
                "\n",
                "from google.colab import files\n",
                "for fn in ['stdcnn_loo_zeroshot_results.json', 'stdcnn_loo_results.png']:\n",
                "    if os.path.exists(fn):\n",
                "        print(f'‚¨áÔ∏è {fn}')\n",
                "        try: files.download(fn)\n",
                "        except: pass"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}