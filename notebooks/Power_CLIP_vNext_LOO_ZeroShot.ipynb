{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ Power-CLIP vNext: Attribute-Level LOO Zero-Shot\n",
                "\n",
                "**ÁúüÊ≠£ÁöÑ Zero-Shot È©óË≠â**\n",
                "\n",
                "ÊØèÊ¨°Áïô‰∏ÄÈ°ûË®≠ÂÇô‰∏çË®ìÁ∑¥ (LOO)ÔºåÊ∏¨Ë©¶ attribute-level ÁöÑË™ûÊÑèÊ≥õÂåñËÉΩÂäõ\n",
                "\n",
                "| Fold | Ë®ìÁ∑¥ (5È°û) | Ê∏¨Ë©¶ (1È°û Unseen) |\n",
                "|------|-----------|-------------------|\n",
                "| LOO-1 | F,M,D,W,S | **H** (heater) |\n",
                "| LOO-2 | F,M,D,W,H | **S** (stove) |\n",
                "| LOO-3 | F,M,D,S,H | **W** (washer) |\n",
                "| LOO-4 | F,M,W,S,H | **D** (dishwasher) |\n",
                "| LOO-5 | F,D,W,S,H | **M** (microwave) |\n",
                "| LOO-6 | M,D,W,S,H | **F** (fridge) |\n",
                "\n",
                "**Ë©ï‰º∞ÊåáÊ®ô (Reviewer-acceptable)**:\n",
                "- Avg Rank vs Random (3.5)\n",
                "- Top-3 Recall vs Random (50%)\n",
                "- Attribute Hit Rate\n",
                "\n",
                "**ÁâàÊú¨**: 2025-12-31 vNext-LOO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìÇ Setup\n",
                "from google.colab import drive, userdata\n",
                "import os, json, torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "import random\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "BASE_PATH = '/content/drive/MyDrive/Reserach/REDD_Dataset'\n",
                "os.chdir(BASE_PATH)\n",
                "\n",
                "SEED = 42\n",
                "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'üî• Device: {DEVICE}')\n",
                "\n",
                "EPOCHS = 20\n",
                "LR = 1e-3\n",
                "BATCH_SIZE = 32\n",
                "TEMPERATURE = 0.07"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q sentence-transformers openai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìã Attribute Ontology (20 prototypes)\n",
                "# Design Principles:\n",
                "# 1. Shareable - attributes are shared across devices\n",
                "# 2. Observable - can be detected from power signals\n",
                "# 3. Minimal - 20 prototypes, not heuristic explosion\n",
                "\n",
                "ATTRIBUTES = {\n",
                "    # Tier 1: Power Shape (5)\n",
                "    'PS1': {'tier': 'power_shape', 'name': 'constant_high', 'text': 'sustained high power consumption above 500W'},\n",
                "    'PS2': {'tier': 'power_shape', 'name': 'constant_low', 'text': 'sustained low power consumption under 200W'},\n",
                "    'PS3': {'tier': 'power_shape', 'name': 'cyclic', 'text': 'power cycles on and off periodically'},\n",
                "    'PS4': {'tier': 'power_shape', 'name': 'burst', 'text': 'short bursts of high power'},\n",
                "    'PS5': {'tier': 'power_shape', 'name': 'fluctuating', 'text': 'continuously fluctuating power levels'},\n",
                "    # Tier 2: Temporal (4)\n",
                "    'T1': {'tier': 'temporal', 'name': 'long_operation', 'text': 'operates continuously for 30+ minutes'},\n",
                "    'T2': {'tier': 'temporal', 'name': 'short_usage', 'text': 'used for less than 10 minutes at a time'},\n",
                "    'T3': {'tier': 'temporal', 'name': 'periodic_cycle', 'text': 'regular on-off cycles every 15-30 minutes'},\n",
                "    'T4': {'tier': 'temporal', 'name': 'instantaneous', 'text': 'very brief operation under 5 minutes'},\n",
                "    # Tier 3: Load Type (5)\n",
                "    'L1': {'tier': 'load_type', 'name': 'resistive', 'text': 'purely resistive heating load with stable power'},\n",
                "    'L2': {'tier': 'load_type', 'name': 'inductive_motor', 'text': 'inductive motor load with startup surge'},\n",
                "    'L3': {'tier': 'load_type', 'name': 'switching', 'text': 'switching power supply with high frequency noise'},\n",
                "    'L4': {'tier': 'load_type', 'name': 'heating_element', 'text': 'resistive heating element with thermal cycling'},\n",
                "    'L5': {'tier': 'load_type', 'name': 'compressor', 'text': 'compressor motor with periodic start-stop'},\n",
                "    # Tier 4: Power Level (4)\n",
                "    'P1': {'tier': 'power_level', 'name': 'very_high', 'text': 'power consumption above 1500W'},\n",
                "    'P2': {'tier': 'power_level', 'name': 'high', 'text': 'power consumption between 500W and 1500W'},\n",
                "    'P3': {'tier': 'power_level', 'name': 'medium', 'text': 'power consumption between 100W and 500W'},\n",
                "    'P4': {'tier': 'power_level', 'name': 'low', 'text': 'power consumption below 100W'},\n",
                "    # Tier 5: Stability (2)\n",
                "    'S1': {'tier': 'stability', 'name': 'stable', 'text': 'very stable and consistent power draw'},\n",
                "    'S2': {'tier': 'stability', 'name': 'variable', 'text': 'highly variable and unpredictable power'},\n",
                "}\n",
                "\n",
                "ATTR_IDS = list(ATTRIBUTES.keys())\n",
                "ATTR_TEXTS = [ATTRIBUTES[k]['text'] for k in ATTR_IDS]\n",
                "\n",
                "DEVICE_ATTRS = {\n",
                "    'fridge': ['L5', 'T3', 'P4', 'PS3', 'S1'],\n",
                "    'microwave': ['L1', 'T4', 'P1', 'PS4', 'S1'],\n",
                "    'dish washer': ['L2', 'L4', 'T1', 'P2', 'PS5'],\n",
                "    'washer dryer': ['L2', 'L4', 'T1', 'P1', 'PS5', 'S2'],\n",
                "    'electric stove': ['L1', 'T2', 'P1', 'PS1', 'S1'],\n",
                "    'electric space heater': ['L4', 'T1', 'P2', 'PS1', 'S1'],\n",
                "}\n",
                "\n",
                "ALL_DEVICES = list(DEVICE_ATTRS.keys())\n",
                "SHORT = {'fridge': 'F', 'microwave': 'M', 'dish washer': 'D', \n",
                "         'washer dryer': 'W', 'electric stove': 'S', 'electric space heater': 'H'}\n",
                "\n",
                "print(f'üìã Attributes: {len(ATTR_IDS)} (5 tiers)')\n",
                "print(f'üìã Devices: {len(ALL_DEVICES)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title ‚ö° Model & Utils\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "class HTFCNNEncoder(nn.Module):\n",
                "    def __init__(self, embed_dim=384):\n",
                "        super().__init__()\n",
                "        self.time_branch = nn.Sequential(\n",
                "            nn.Conv1d(1, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
                "            nn.Conv1d(64, 128, 5, padding=2), nn.BatchNorm1d(128), nn.ReLU(), nn.AdaptiveAvgPool1d(1)\n",
                "        )\n",
                "        self.freq_branch = nn.Sequential(\n",
                "            nn.Conv1d(1, 64, 3, padding=1), nn.BatchNorm1d(64), nn.ReLU(), nn.AdaptiveAvgPool1d(1)\n",
                "        )\n",
                "        self.fusion = nn.Sequential(\n",
                "            nn.Linear(128 + 64 + 6, 256), nn.LayerNorm(256), nn.ReLU(), nn.Dropout(0.2),\n",
                "            nn.Linear(256, embed_dim)\n",
                "        )\n",
                "    \n",
                "    def forward(self, signal, stats):\n",
                "        mean, std = signal.mean(1, keepdim=True), signal.std(1, keepdim=True) + 1e-6\n",
                "        norm = (signal - mean) / std\n",
                "        time_f = self.time_branch(norm.unsqueeze(1)).squeeze(-1)\n",
                "        fft = torch.abs(torch.fft.rfft(norm, dim=1))\n",
                "        fft = fft / (fft.max(1, keepdim=True)[0] + 1e-6)\n",
                "        freq_f = self.freq_branch(fft.unsqueeze(1)).squeeze(-1)\n",
                "        return self.fusion(torch.cat([time_f, freq_f, stats], dim=1))\n",
                "\n",
                "def compute_stats(s):\n",
                "    return np.array([np.mean(s), np.std(s), np.max(s)-np.min(s), np.max(s), np.min(s),\n",
                "                     np.max(np.abs(np.diff(s))) if len(s)>1 else 0], dtype=np.float32)\n",
                "\n",
                "print(f'‚úÖ Model: {sum(p.numel() for p in HTFCNNEncoder().parameters()):,} params')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìä Load All Data\n",
                "\n",
                "all_samples = []\n",
                "ws = 60\n",
                "with open('combination_labels.jsonl') as f:\n",
                "    lines = f.readlines()\n",
                "\n",
                "for i in tqdm(range(0, len(lines)-ws, ws), desc='Loading'):\n",
                "    chunk = [json.loads(lines[j]) for j in range(i, i+ws)]\n",
                "    act = set(chunk[ws//2]['active_appliances']) & set(ALL_DEVICES)\n",
                "    if not act: continue\n",
                "    \n",
                "    attr_set = set()\n",
                "    for d in act:\n",
                "        attr_set.update(DEVICE_ATTRS.get(d, []))\n",
                "    if not attr_set: continue\n",
                "    \n",
                "    signal = np.array([c['aggregate_power'] for c in chunk], dtype=np.float32)\n",
                "    attr_mask = np.zeros(len(ATTR_IDS), dtype=np.float32)\n",
                "    for attr in attr_set:\n",
                "        attr_mask[ATTR_IDS.index(attr)] = 1.0\n",
                "    \n",
                "    all_samples.append({\n",
                "        'signal': signal,\n",
                "        'stats': compute_stats(signal),\n",
                "        'attr_mask': attr_mask,\n",
                "        'devices': list(act),\n",
                "        'attrs': list(attr_set)\n",
                "    })\n",
                "\n",
                "print(f'üìä Total samples: {len(all_samples)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üéØ LOO Attribute Dataset\n",
                "\n",
                "class LOOAttrDataset(Dataset):\n",
                "    def __init__(self, samples, left_out_device, is_train=True):\n",
                "        if is_train:\n",
                "            # Exclude samples containing left_out_device\n",
                "            self.samples = [s for s in samples if left_out_device not in s['devices']]\n",
                "        else:\n",
                "            # Only pure left_out_device samples\n",
                "            self.samples = [s for s in samples \n",
                "                           if left_out_device in s['devices'] and len(s['devices']) == 1]\n",
                "        self.left_out = left_out_device\n",
                "    \n",
                "    def __len__(self): return len(self.samples)\n",
                "    def __getitem__(self, i):\n",
                "        s = self.samples[i]\n",
                "        return (torch.tensor(s['signal']), torch.tensor(s['stats']), \n",
                "                torch.tensor(s['attr_mask']), s['devices'], s['attrs'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üî• Train & Evaluate One LOO Fold\n",
                "\n",
                "def run_loo_fold(left_out, all_samples, epochs=EPOCHS):\n",
                "    print(f'\\n{\"=\"*60}')\n",
                "    print(f'üéØ LOO Attribute: Leave out [{SHORT[left_out]}] = {left_out}')\n",
                "    print(f'{\"=\"*60}')\n",
                "    \n",
                "    train_ds = LOOAttrDataset(all_samples, left_out, is_train=True)\n",
                "    test_ds = LOOAttrDataset(all_samples, left_out, is_train=False)\n",
                "    \n",
                "    print(f'Train: {len(train_ds)} (without {SHORT[left_out]})')\n",
                "    print(f'Test: {len(test_ds)} (only {SHORT[left_out]})')\n",
                "    \n",
                "    if len(test_ds) == 0:\n",
                "        return None\n",
                "    \n",
                "    # Model & Text Encoder\n",
                "    model = HTFCNNEncoder().to(DEVICE)\n",
                "    text_model = SentenceTransformer('all-MiniLM-L6-v2').to(DEVICE)\n",
                "    text_model.eval()\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        attr_embs = text_model.encode(ATTR_TEXTS, convert_to_tensor=True, device=DEVICE)\n",
                "        attr_embs = F.normalize(attr_embs, p=2, dim=1)\n",
                "    \n",
                "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
                "    \n",
                "    def collate(batch):\n",
                "        return (torch.stack([b[0] for b in batch]), \n",
                "                torch.stack([b[1] for b in batch]),\n",
                "                torch.stack([b[2] for b in batch]),\n",
                "                [b[3] for b in batch], [b[4] for b in batch])\n",
                "    \n",
                "    loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
                "                        drop_last=True, collate_fn=collate)\n",
                "    \n",
                "    # Training\n",
                "    for ep in range(epochs):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        for signals, stats, masks, _, _ in loader:\n",
                "            signals, stats, masks = signals.to(DEVICE), stats.to(DEVICE), masks.to(DEVICE)\n",
                "            pe = F.normalize(model(signals, stats), p=2, dim=1)\n",
                "            sim = (pe @ attr_embs.T) / TEMPERATURE\n",
                "            exp_sim = torch.exp(sim)\n",
                "            pos_sum = (exp_sim * masks).sum(dim=1)\n",
                "            all_sum = exp_sim.sum(dim=1)\n",
                "            loss = -torch.log(pos_sum / (all_sum + 1e-8)).mean()\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            optimizer.step()\n",
                "            total_loss += loss.item()\n",
                "        if (ep+1) % 10 == 0:\n",
                "            print(f'  Ep {ep+1}/{epochs} Loss: {total_loss/len(loader):.4f}')\n",
                "    \n",
                "    # Evaluation\n",
                "    model.eval()\n",
                "    device_ranks, attr_hits, attr_precs = [], [], []\n",
                "    target_attrs = set(DEVICE_ATTRS[left_out])\n",
                "    \n",
                "    for i in range(len(test_ds)):\n",
                "        signal, stats, gt_mask, _, gt_attrs = test_ds[i]\n",
                "        signal = signal.unsqueeze(0).to(DEVICE)\n",
                "        stats = stats.unsqueeze(0).to(DEVICE)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            pe = F.normalize(model(signal, stats), p=2, dim=1)\n",
                "            sim = (pe @ attr_embs.T).squeeze().cpu().numpy()\n",
                "        \n",
                "        # Attribute activation\n",
                "        activations = 1 / (1 + np.exp(-sim * 5))\n",
                "        \n",
                "        # Device reasoning\n",
                "        device_scores = {}\n",
                "        for dev, expected in DEVICE_ATTRS.items():\n",
                "            indices = [ATTR_IDS.index(a) for a in expected]\n",
                "            device_scores[dev] = np.mean([activations[j] for j in indices])\n",
                "        \n",
                "        sorted_devs = sorted(device_scores.items(), key=lambda x: -x[1])\n",
                "        rank = [d[0] for d in sorted_devs].index(left_out) + 1\n",
                "        device_ranks.append(rank)\n",
                "        \n",
                "        # Attribute hit: how many target attrs in top-K predictions?\n",
                "        top_k_attrs = set([ATTR_IDS[j] for j in np.argsort(-activations)[:len(target_attrs)]])\n",
                "        hit = len(top_k_attrs & target_attrs) / len(target_attrs)\n",
                "        attr_hits.append(hit)\n",
                "    \n",
                "    avg_rank = np.mean(device_ranks)\n",
                "    top1 = np.mean([r == 1 for r in device_ranks]) * 100\n",
                "    top3 = np.mean([r <= 3 for r in device_ranks]) * 100\n",
                "    attr_hit_rate = np.mean(attr_hits) * 100\n",
                "    \n",
                "    print(f'\\nüìä {SHORT[left_out]} ({left_out}):')\n",
                "    print(f'   Rank: {avg_rank:.2f} (random=3.5)')\n",
                "    print(f'   Top-1: {top1:.1f}%, Top-3: {top3:.1f}%')\n",
                "    print(f'   Attr Hit Rate: {attr_hit_rate:.1f}%')\n",
                "    \n",
                "    return {\n",
                "        'device': left_out,\n",
                "        'short': SHORT[left_out],\n",
                "        'n_train': len(train_ds),\n",
                "        'n_test': len(test_ds),\n",
                "        'avg_rank': avg_rank,\n",
                "        'top1': top1,\n",
                "        'top3': top3,\n",
                "        'attr_hit_rate': attr_hit_rate\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üöÄ Run All 6 LOO Folds\n",
                "\n",
                "results = []\n",
                "for device in ALL_DEVICES:\n",
                "    result = run_loo_fold(device, all_samples, epochs=EPOCHS)\n",
                "    if result:\n",
                "        results.append(result)\n",
                "\n",
                "print('\\n' + '=' * 60)\n",
                "print('üìä ALL LOO FOLDS COMPLETE')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìä Summary Table (Reviewer-ready)\n",
                "\n",
                "print('\\n### Attribute-Level LOO Zero-Shot Results\\n')\n",
                "print('| Device | Test | Avg Rank | Top-1 | Top-3 | Attr Hit |')\n",
                "print('|--------|------|----------|-------|-------|----------|')\n",
                "\n",
                "for r in results:\n",
                "    better_rank = '‚úÖ' if r['avg_rank'] < 3.5 else '‚ùå'\n",
                "    better_top3 = '‚úÖ' if r['top3'] > 50 else '‚ùå'\n",
                "    print(f\"| {r['short']} ({r['device']}) | {r['n_test']} | {r['avg_rank']:.2f} {better_rank} | {r['top1']:.1f}% | {r['top3']:.1f}% {better_top3} | {r['attr_hit_rate']:.1f}% |\")\n",
                "\n",
                "avg_rank = np.mean([r['avg_rank'] for r in results])\n",
                "avg_top1 = np.mean([r['top1'] for r in results])\n",
                "avg_top3 = np.mean([r['top3'] for r in results])\n",
                "avg_attr = np.mean([r['attr_hit_rate'] for r in results])\n",
                "\n",
                "print(f'| **Average** | - | **{avg_rank:.2f}** | **{avg_top1:.1f}%** | **{avg_top3:.1f}%** | **{avg_attr:.1f}%** |')\n",
                "print(f'| Random | - | 3.5 | 16.7% | 50% | - |')\n",
                "\n",
                "print(f'\\nüéØ **Summary**:')\n",
                "print(f'   Avg Rank: {avg_rank:.2f} vs Random 3.5 ‚Üí {\"‚úÖ Better\" if avg_rank < 3.5 else \"‚ùå Worse\"}')\n",
                "print(f'   Top-3: {avg_top3:.1f}% vs Random 50% ‚Üí {\"‚úÖ Better\" if avg_top3 > 50 else \"‚ùå Worse\"}')\n",
                "print(f'   Attr Hit: {avg_attr:.1f}% (new metric)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üìà Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "devices = [r['short'] for r in results]\n",
                "ranks = [r['avg_rank'] for r in results]\n",
                "top3s = [r['top3'] for r in results]\n",
                "hits = [r['attr_hit_rate'] for r in results]\n",
                "\n",
                "# Rank\n",
                "colors = ['green' if r < 3.5 else 'red' for r in ranks]\n",
                "axes[0].bar(devices, ranks, color=colors, alpha=0.7)\n",
                "axes[0].axhline(y=3.5, color='black', linestyle='--', label='Random (3.5)')\n",
                "axes[0].set_ylabel('Avg Rank (‚Üì better)')\n",
                "axes[0].set_title('Device Rank')\n",
                "axes[0].legend()\n",
                "\n",
                "# Top-3\n",
                "colors = ['green' if t > 50 else 'red' for t in top3s]\n",
                "axes[1].bar(devices, top3s, color=colors, alpha=0.7)\n",
                "axes[1].axhline(y=50, color='black', linestyle='--', label='Random (50%)')\n",
                "axes[1].set_ylabel('Top-3 Recall (%)')\n",
                "axes[1].set_title('Top-3 Recall')\n",
                "axes[1].legend()\n",
                "\n",
                "# Attr Hit\n",
                "axes[2].bar(devices, hits, color='blue', alpha=0.7)\n",
                "axes[2].set_ylabel('Attribute Hit Rate (%)')\n",
                "axes[2].set_title('Attribute Retrieval')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('vnext_loo_results.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title üíæ Save Results\n",
                "\n",
                "final_results = {\n",
                "    'experiment': 'Attribute-Level LOO Zero-Shot',\n",
                "    'epochs': EPOCHS,\n",
                "    'n_attributes': len(ATTR_IDS),\n",
                "    'folds': results,\n",
                "    'average': {\n",
                "        'rank': avg_rank,\n",
                "        'top1': avg_top1,\n",
                "        'top3': avg_top3,\n",
                "        'attr_hit_rate': avg_attr\n",
                "    },\n",
                "    'vs_random': {\n",
                "        'rank_better': avg_rank < 3.5,\n",
                "        'top3_better': avg_top3 > 50\n",
                "    }\n",
                "}\n",
                "\n",
                "with open('vnext_loo_zeroshot_results.json', 'w') as f:\n",
                "    json.dump(final_results, f, indent=2)\n",
                "\n",
                "from google.colab import files\n",
                "for fn in ['vnext_loo_zeroshot_results.json', 'vnext_loo_results.png']:\n",
                "    if os.path.exists(fn):\n",
                "        print(f'‚¨áÔ∏è {fn}')\n",
                "        try: files.download(fn)\n",
                "        except: pass"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}